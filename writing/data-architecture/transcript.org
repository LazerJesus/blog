### i gave a presentation on my startups data architecture where i present something i call the 5s architecture data processing model. now i want to write this presentation into an article. summarize the sections for me

you take everybody onto my screen so I'm as I've already tasted a little bit that's going to be multiple presentations over the next couple of weeks and we are going to start today with what we have built and I kind of have a metaphorical narrative that I go through as well as the actual thing that we work with at the same time next time we are going to talk about how does it work so today is what have
 we built next time how does it work how does it is actually what is the mechanics and then there's the were the last one is going to be Where Do We Go From Here what is actually the things that we will build on top of this and there's going to be the first kind of internal cool fencer internal product announcement we have a little data product coming up and that is going to be revolutionary not just going to be a nice little ketchup it is really so today I'm going
 try to keep it short I've done the presentation and I tend to talk a lot because I love this topic I'm trying to keep it short the purpose is to give you an overview of the inner workings of the data team of the data system such that you get a realistic understanding of what's possible and what we can actually Implement for you I want to give you a language and tools such that you can ask questions for us because we as a data team we are in a supporting role we are here to make all of you what more work more effectively
 and yeah we need collaboration for this so scope is going I'm going to talk you once through our kind of the infrastructure that we have how we work with the data what operations we perform on it and then what tools you have available to you to consume that data
 and
 yeah I think with that I'm just going to do a high-level overview of what we're going to cover just today today is kind of the foundational layer as I've already said next time will be the core part but it's really nice to have heard and understood the process that we go through and so that's what we're going to do today so data is everywhere we have it all over the planet data that we want to have access to we need to have access to that and we need to
 XS which regularly and reliably once we have access to it once we control the we want to shape it put it in the shape that we want to have it means combinations reductions of that stuff and then we need to actually extract useful work from it and something that is also that goes through the exact same process is basically all kind of land resources so let's take oil for example there is oil over the all over the planet or theirs
 like a Colles all over the planet and what you do when you have that for us the the metaphor would be we have data at different places we have it in HubSpot where we have information about our channels from growth leads companies deals that is a sales funnel that we have we have the sales activities that actually go and picking up the phone and dialing all of that stuff that comes from HubSpot and from some peripheral like peripherals like are called
 we have our user Behavior data click button X rerender create project sign up and you will person that all of that is coming mrs. user Behavior coming from the various products that we have three at the moment we front-ends someone is doing something in the background and then we have product data such as loading times crashes pick a thing releases also interesting because we might want to know


 know if a user Behavior change of the release now we have a bunch of stuff in spreadsheets which is chaotic and we are we love things not in spreadsheets but it's the easiest way to integrate something quickly an example here is financial data the marketing spend would be an example for that if we want to calculate what it costs to acquire contracts also something that we still have and spreadsheets yeah and then there's like Sanskrit people working with zen garden
 what they're using it for type form a trio rolls all over the planet so the core message is data is everywhere it's stored like deep in the ground we can't come to it we can't do useful work with it so we need a way to access it specifically we need a way to extract it we need to get it out of the ground we need to transport it we need to get it to the point that we can control and then once it's there we want to store it because we don't work with that and we have a huge amount
 of data like actually not a huge not Google but we have a sizable and so what we do is this is a little bit on the nerdy side but the programmers will will value this we have for example I'm going through the example on how to spot the contacts we send a get request to HubSpot please tell us about this little bit of please give us all of the contacts that you have they respond with a
 some array of all of the contacts and we take that and we store it into our little warehouse for that we use singer singers that kind of technology that has implemented a universal kind of anonymous data format everything that we get is my is translated into this Universal data format and then we can put it into somewhere else has a kind of a pit stop thing yeah yeah
 we need to in the middle keep track of what is the last contact that we have loaded how many contacts were created in the meantime please only load those contacts because of course we don't want to extract the whole mine every time we go there just like the changes we have storage somewhere and that is a big query so taking the nerd cap off again we now have we have our data we have extracted we
 got everything that we want from HubSpot into our storage where we can control it where we can one second
 alright perfect so now we have that data in our storage system when we don't only want to do this once we want to do this continuously and on time and if it doesn't happen we want to have some controls over it like restart run it again run it again from certain point in time and that we do with air flow which is this beautiful little super complicated tool that we can we can express if I tasks that we do regularly for example load
 oh doll HubSpot data and we run that once a day we run that every hour we can Define that and then gives us an overview did run was it successful that explode can we can also add some scripts that we can run and afterwards so for example if you want to test it with Great Expectations we could do that in airflow and now that we have our data loaded and that continues the we want to make
 it reliable it's really important that we are actually trust our data and so we first off we tested while it's processing in what that's flowing through our pipes and afterwards we inspect and we make sure that there's there's a certain reasonable assumptions that we can make about the data in code that is just this is how it's implemented we have this database that we store all of the accounts to so the coupons are accounts of the users would be another and we can specify that we think that the most recent




 recent data in the accounts should be no older than one day that means if that is true then we know that we have loaded our data successfully there's other assumptions that we can make there's a bunch of them for example the first instance of a data should be somewhere 2019 2020 depending on the type of data the most recent should be within a couple of days we know some specific entrances should be present for example the co-founder user should be there or
 like we know by now that I'm free on its customers we can hard code that yeah we know that there's some relationships between tables should be there for example if we have something specified in the deal an account specified in a deal than that account should be present in our product database so we can specify the relations between HubSpot and our product we do all of that and if something fails then we're going to see that an airflow
 all right with that being said we already have extracted our data we are piping it we are moving it to the place that we want to be doing that regularly and we are sure that our data is not complete and utter utter garbage and so now we need to shape it and this is actually the Crux of the matter this is the most important the most important and the most interesting part of our of our work here all of the
 beauty and Magic happens so it's kind of like comparably if you would think back to the resource example once you have extracted oil for place and you have transported it and you've stored it you want to do some useful work on it so you want to heat it to a certain temperature to split out some trash you will want to extract the gases and you have this whole like destination process and then things
 like a Distillery and it's a very complex process and that exactly is the same thing that we have implemented just on a data level from what we are doing I'm just going to run you a teeny tiny bit through this example nerd hat back on
 what this example shows us here we are operating on the deal data so the deals that we receive from HubSpot which we get from HubSpot we get the deals and then when we do two operations on that we first we associate the owner the deal owner from from our sales team and the qualifier from the pr team and so when we when everyone anyone reads the
deal to adjust any of you analyzing the deal data you now have access to the person who qualified that deal we have of course the score associated with it but we also know the person that qualified it and the person that is currently operating in its on it which is the owner name and usually that is two different names and one special case now that is one name and that is Marco because he transitioned from some other qualifies to some of that is also an owner actually broke
 there was a test there that we had to remove yeah and that this this kind of operation so combining two different tables and reading data from it a combining it is something that we do in DB T that is where the actual magic happens DBT gives us the the all of the processes that we need to make this happen and
I'm going to go into some detail or we when Carlos back we're going to go into some detail about this just a little bit of a teaser There is five stages general areas that we do in their distinct from each other the first is storage now we have sourcing staging shaping and surface each one of those storage is here resides the raw data so all of the crap that we get from all over the world is just piped into this
 this into this storage space it it's an ugly place it is a chaotic place and we as data we are completely happy that it is that you can just dump anything in there because we have a clean sourcing process and that is we pull the data that we want from that chaotic place so we're sourcing we pull from Storage so we have reduce the amount of data that we have to add work on staging here we make sure that all of the names are in place because the growth team
 the sometimes different names than the sales team then the product team then the customer success team and we as they do we really want things to have a unified schema unified naming conventions and that is what we do in staging and then shaping here we do the Rubik's Cube operations on our data for example a project can be rendered multiple times and will be nice to have the information how many times a project was rendered already on the project without anyone having to



do some work here so this is kind of a Rubik's Cube operation will be look at the amount of render events that happened for a project and we reduce it down and just count we reduce it to a number so Project X was rendered y times and that's the kind of stuff that we can do in DBT and it is a absolutely mind-boggling transformative piece of technology and fell I'm not going to talk about that it's actually even more interesting because here we can run artificial intelligence directly in our warehouse
 which is well when I think about it I get like equally with that being said we now have our okay and the last one sorry I forgot one surface so the data is shaped and now we have our services where we provide the data to someone all of this what's happening storage everybody is writing in
 sirs surface everybody's the reading out that is where metal base is reading that is where the tools that we are building on top of our data infrastructure will be built on what readout and that is where the customer success team the sales team and they're all of you are reading your data from and that is the surface so we have our data from everywhere access to regularly the it's there's reliability baked into the system we can shape it minivan
 nearly as we want to on the Fly we are extremely powerful when it comes to this and now we need to extract useful work from and we make the city blow we need to transform the energy into light and we need to put something as beautiful as Paris and one way that we do this is by working with all of you having a conversation and we facilitate that in our
 data open Office hours everybody please feel invited we've had these a couple of times now we had them on Friday and now the moved to Wednesday it's a one and a half hour session where everybody that has a question once you learn something about data wants to become competent or have a specific questions answered or is stuck in a questions or pick a thing you can come Carla me and in the future but a tire and then voters are going to be there for you and we will
 help you find your answer whatever it is if it's a data also if we don't have something in that point we can not down build it out in the next time next open Office hours we will have the answer so please just join it in my contacts and calendars cards calendar you can just join and then we have a tool that we've already set up as metal base I think all of you have heard about it for example to go
we were a couple of interesting Data Insights all of those come from metal base all of the dashboards are built there and the the tool that we are building is actually built kind of around metal base and it's going to be really fancy yeah so that was kind of the super quick and dirty of it with this I have given you an overview of how we work on
 an infrastructure level that is the thing that we needed as a ground layer to know do useful work and to go next time into more specifics around the actual data that we're getting from the different sources and then the the work and the insights and the kpis that we compute from that of course this is never done always working progress if anyone has ideas that will be over shoot through the more work we have the channel
 so as a last thing before I leave the before I grab a mic and you can ask some questions we can see for example that the mq else per week we have a steady upward Trend like a really nice lower left to upper right slope its kind of consistent in terms of mql that are generated per week and the quality is consistent it's hovering at the moment around an average score
 score for the mql that are generated or for going down to 3.5 6 point something there's a range here but we are kind of consistent the quality and Olivia told me earlier that the huge like a spike that we see in that should be end of April probably is due to some apps but perhaps but cleaning process that we had I'm sure all of you remember
 I'm actually Unearthed a lot a lot of mql that hasn't been scored yet and we were able to score those and have a huge influx into our pipeline because of that
 then we have a really amazing news and that is the sqls per month per vdr we see that with some consistent performance and Kim is joined this month and is doing just absolute damage the month isn't even over and she's leading the scoreboard which is I would personally exactly that is what she's doing we get a I would love to know your secret how are you so performance so far



but yeah there we go then we have the contact lifecycle conversion for all sources we see that of the about seven 18,000 contacts that were in the stages of lead and subscriber of those 6200 became marketing qualified leads 500 because of sales qualified leads and of those four and we came to opportunities at least according to the data that we have in app spot now interestingly because we can also associate the
 channel data because we have a holistic system you can also associate the channel data to what have actually happens in terms of conversion and I only look at the LinkedIn box we see that the conversion rates the success rates for people that we are cry acquire with our leads that we acquire with all links and but the conversion rates are substantially higher
 and I can but we can break down the sources in even even more nitty-gritty and we have all of the sources available email leads or Warmack was encoded with a growth knows all of them I don't have them with the tip of my tongue at the moment so email marketing director affecting offline organic search paid search paid social there's a lot of those at them open as a
 tap McKinney so and yeah we see that for example the LinkedIn but performs really well someone wants to compare that to pay social please be my guest we have this year alone we had a really consistent and substantial increase in user activity January February March April May June June isn't over yet so the six-month is to be taken with a grain of salt that could still increase and decrease though and what it means is that
 I don't know what we did but what we see in data is that user in activity is decreasing it has been haft and one other thing that we see from the product side which is very interesting is that the renders per account the so we have an account and how many times they render it total in a given week that is increasing but the amount of average renders per project
 is decreasing so their rendering more but they don't need to re-render as many times I would assume that they become that either our product we're both our product come becoming more easy to use as well as our users are becoming more familiar with us they know how we work and it becomes easier for them to produce a video and I am absolutely excited to see what will happen once we have our new render engine
 that is going to be insane with electric previous and stuff
 cook that is like a little bit of an Insight doing the plug again open Office hours every Wednesday join us we can't help you if you don't come and ask for its there's usually a lot of interesting stuff that happens in that time frame and with that being said I am done and over with my little rundown of how the data Department
 is working of our infrastructure is working and what trends we see
 if you have any questions
 raise your hands




 alright usual substance suspect number one first first of all Kim amazing amazing job you're doing no first of all finding amazing what you did was call you as well very nice to see and I'm just very curious also cautious about the SQL pepper PDR because the numbers they are not
 completely right they are in the right direction yes but I double checked it and they are not 100% right it's not about that I just want I just want you to know that there is no 100% proof at the moment we can talk about that later I just wanted to mention it so there's always a thing and that's why we need the office hours is a lot about definitions how do you define some and a qualified SQL versus an mqa
 LL and there is a lot of conversation happening around this and that is probably where this discrepancy is coming from we've built a query that looks for specific criteria and that is informing this graph and in HubSpot you probably have another set of criteria that is defining the graph so it is really important to be diligent with the definitions here that is the Crux of the matter that is where most of the work is actually happening and that's why we need the conversation and that's why we
 the conversation to never stop because you learn new things you change your criteria and then it like this describe it lets your dress let's have a chat here Pi is this amazing
 yeah and I'm especially looking forward to Britta who's going to take ownership of this in the future and with that being said Caesar
 yeah thanks I wanted to ask if we can scrape external channels as data sources for example for example like analyze people's behavior on LinkedIn
 absolutely possible that would just be another type of source that we would have to Define and probably some tooling to build the actual analysis of that it is not something that is a low-hanging fruit because we would have to build like a whole like lingering scraper for that or buy a tool that does that I definitely it is interesting so really interesting and once we have the data there's a lot more we could
 could do with this there's yeah it's not a low-hanging fruit but it's possible alright see you in the office hours then all right and I also had Tom here for a second but he seems to have eat it the premises but I just like quick questions around you say data is everywhere but it doesn't seem that Canada is very appreciative of data
 I heard that I heard that on the internet okay well there's two things here the first that's a resource map and well in geography replace a parole the second one is I would guess that also in Sahara there's a bunch of resources we just haven't looked because it's really hard to look there so we just don't have someone going there with a huge giant shovel digging until they struck gold or oil
 because well you burn up in the meantime so we can't tell if there's a resource when we haven't looked whether there's a resource so Canada I am completely bullish on your recent City
 go go Canada what is the national animal of Canada two keys or something Deva means busto the beaver is one of them we have to
 don't be rude I'm a fan of the snowy owl that's the bird of Quebec you know the park go go ahead with the rest of Canada go ahead Wick
 you know
 all right in that case I unless there's any more questions 32 morning it's a good show
